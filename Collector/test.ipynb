{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32evtlog\n",
    "import sys\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_level_dict = {0 : 'LogAlways',\n",
    "                1 : 'Critical',\n",
    "                2 : 'Error',\n",
    "                3 : 'Warning',\n",
    "                4 : 'Informational',\n",
    "                5 : 'Verbose'}\n",
    "\n",
    "evt_opcode_dict = { 0 : 'Info',\n",
    "                1: 'Start',\n",
    "                2: 'Stop',\n",
    "                3: 'DataCollectionStart',\n",
    "                4: 'DataCollectionStop',\n",
    "                5: 'Extension',\n",
    "                6: 'Reply',\n",
    "                7: 'Resume',\n",
    "                8: 'Suspend',\n",
    "                9: 'Send',\n",
    "                240: 'Receive'}\n",
    "\n",
    "\n",
    "def parse_XML_log(event):\n",
    "    tree = ET.ElementTree(ET.fromstring(event))\n",
    "    root = tree.getroot()\n",
    "    ns = \"{http://schemas.microsoft.com/win/2004/08/events/event}\"\n",
    "    data = {}\n",
    "    for eventID in root.findall(\".//\"):\n",
    "        if eventID.tag == f\"{ns}System\":\n",
    "            for e_id in eventID.iter():\n",
    "                if e_id.tag == f\"{ns}System\":\n",
    "                    pass\n",
    "                elif e_id.tag == f\"{ns}Provider\":\n",
    "                    data[\"Provider\"] = e_id.attrib.get('Name')\n",
    "                elif e_id.tag == f\"{ns}TimeCreated\":\n",
    "                    data[\"TimeCreated\"] = e_id.attrib.get('SystemTime')\n",
    "                elif e_id.tag == f\"{ns}Correlation\":\n",
    "                    data[\"ActivityID\"] = e_id.attrib.get('ActivityID')\n",
    "                elif e_id.tag == f\"{ns}Execution\":\n",
    "                    data[\"ProcessID\"] = e_id.attrib.get('ProcessID')\n",
    "                    data[\"ThreadID\"] = e_id.attrib.get('ThreadID')\n",
    "                elif e_id.tag == f\"{ns}Level\":\n",
    "                    if not int(e_id.text) in evt_level_dict.keys():\n",
    "                        data['Level'] = \"unknown\"\n",
    "                    else:\n",
    "                        data['Level'] = evt_level_dict[int(e_id.text)]\n",
    "                elif e_id.tag == f\"{ns}Opcode\":\n",
    "                    if not int(e_id.text) in evt_opcode_dict.keys():\n",
    "                        data['Opcode'] = \"unknown\"\n",
    "                    else:\n",
    "                        data['Opcode'] = evt_opcode_dict[int(e_id.text)]\n",
    "                else:\n",
    "                    att = e_id.tag.replace(f\"{ns}\", \"\")\n",
    "                    data[att] = e_id.text\n",
    "\n",
    "        if eventID.tag == f\"{ns}EventData\":\n",
    "            for attr in eventID.iter():\n",
    "                if attr.tag == f'{ns}Data':\n",
    "                    if attr.get('Name') is None:\n",
    "                        data[\"Data\"] = attr.text\n",
    "                    else:\n",
    "                        data[attr.get('Name')] = attr.text\n",
    "                elif attr.tag == f'{ns}Binary':\n",
    "                    data[\"Binary\"] = attr.text\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = { \"info\": \"this object is always passed to your callback\" }\n",
    "# Event log source to listen to\n",
    "event_source = 'application'\n",
    "\n",
    "def new_logs_event_handler(reason, context, evt):\n",
    "    \"\"\"\n",
    "    Called when new events are logged.\n",
    "\n",
    "    reason - reason the event was logged?\n",
    "    context - context the event handler was registered with\n",
    "    evt - event handle\n",
    "    \"\"\"\n",
    "    # Just print some information about the event\n",
    "    # print ('reason', reason, 'context', context, 'event handle', evt)\n",
    "\n",
    "    event = win32evtlog.EvtRender(evt, win32evtlog.EvtRenderEventXml)\n",
    "    result = \" \".join(l.strip() for l in event.splitlines())\n",
    "    print(result)\n",
    "    with open('xml_logs.txt', 'a') as file:\n",
    "        file.write(result)\n",
    "        file.write('\\n')\n",
    "    # parse_XML_log(event=event)\n",
    "    print(' - ')\n",
    "\n",
    "    # Make sure all printed text is actually printed to the console now\n",
    "    sys.stdout.flush()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to future events\n",
    "subscription1 = win32evtlog.EvtSubscribe(event_source, win32evtlog.EvtSubscribeToFutureEvents, None, Callback=new_logs_event_handler, Context=event_context, Query=None)\n",
    "subscription2 = win32evtlog.EvtSubscribe('system', win32evtlog.EvtSubscribeToFutureEvents, None, Callback=new_logs_event_handler, Context=event_context, Query=None)\n",
    "subscription3 = win32evtlog.EvtSubscribe('Security', win32evtlog.EvtSubscribeToFutureEvents, None, Callback=new_logs_event_handler, Context=event_context, Query=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"xml_logs.txt\", \"r+\") as file1:\n",
    "    lines = file1.readlines()   \n",
    "    for line in lines:              \n",
    "        data.append(parse_XML_log(line))   \n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['Windows_Event_Logs']\n",
    "collection = db['mycollection']\n",
    "\n",
    "\n",
    "# Insert the list into the collection\n",
    "\n",
    "collection.insert_many(data)\n",
    "\n",
    "# Print the inserted documents\n",
    "for doc in collection.find():\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n",
      "Data inserted successfull!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCould not connect to MongoDB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m my_consumer:\n\u001b[1;32m     41\u001b[0m     message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mvalue\n\u001b[1;32m     42\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "from pymongo import MongoClient\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "\n",
    "def read_config():\n",
    "    data = {}\n",
    "    config = ConfigParser()\n",
    "    config.read('../kafka_config.ini')\n",
    "    data['topic_name'] = config.get('kafka', 'topic_name')\n",
    "    data['bootstrap_servers'] = config.get('kafka', 'bootstrap_servers')\n",
    "    data['group_id'] = config.get('kafka', 'group_id')\n",
    "    data['auto_offset_reset'] = config.get('kafka', 'auto_offset_reset')\n",
    "    data['enable_auto_commit'] = config.get('kafka', 'enable_auto_commit')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "kafka_config = read_config()\n",
    "# generating the Kafka Consumer\n",
    "my_consumer = KafkaConsumer(\n",
    "    kafka_config['topic_name'],\n",
    "    bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "    auto_offset_reset=kafka_config['auto_offset_reset'],\n",
    "    enable_auto_commit=kafka_config['enable_auto_commit'],\n",
    "    group_id=kafka_config['group_id'],\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    my_client = MongoClient('127.0.0.1', 27017)\n",
    "    my_collection = my_client.my_application.users\n",
    "    print(\"Connected successfully!\")\n",
    "except:\n",
    "    print(\"Could not connect to MongoDB\")\n",
    "\n",
    "\n",
    "for message in my_consumer:\n",
    "    message = message.value\n",
    "    try:\n",
    "        my_collection.insert_one(message)\n",
    "        print(\"Data inserted successfull!\")\n",
    "    except:\n",
    "        print(\"Could not insert into database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from time import sleep\n",
    "import win32evtlog\n",
    "import sys\n",
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "import xmltodict\n",
    "import msvcrt\n",
    "import pyuac\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import os\n",
    "\n",
    "\n",
    "event_context = {\"info\": \"this object is always passed to your callback\"}\n",
    "\n",
    "\n",
    "def read_config(file_name):\n",
    "    data = {}\n",
    "    config = ConfigParser()\n",
    "    config.read(filenames=file_name)  # reading config from file\n",
    "    data['topic_name'] = config.get('kafka', 'topic_name')\n",
    "    data['bootstrap_servers'] = config.get('kafka', 'bootstrap_servers')\n",
    "    data['group_id'] = config.get('kafka', 'group_id')\n",
    "    data['auto_offset_reset'] = config.get('kafka', 'auto_offset_reset')\n",
    "    data['enable_auto_commit'] = config.get('kafka', 'enable_auto_commit')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def xml_to_json(xml_string):\n",
    "    xml_dict = xmltodict.parse(xml_string)\n",
    "    json_data = json.dumps(xml_dict)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "evt_level_dict = {0: 'LogAlways',\n",
    "                  1: 'Critical',\n",
    "                  2: 'Error',\n",
    "                  3: 'Warning',\n",
    "                  4: 'Informational',\n",
    "                  5: 'Verbose'}\n",
    "\n",
    "evt_opcode_dict = {0: 'Info',\n",
    "                   1: 'Start',\n",
    "                   2: 'Stop',\n",
    "                   3: 'DataCollectionStart',\n",
    "                   4: 'DataCollectionStop',\n",
    "                   5: 'Extension',\n",
    "                   6: 'Reply',\n",
    "                   7: 'Resume',\n",
    "                   8: 'Suspend',\n",
    "                   9: 'Send',\n",
    "                   240: 'Receive'}\n",
    "\n",
    "\n",
    "def parse_XML_log(event):\n",
    "    \"\"\"\"Parse a Windows event log entry in XML format into a dictionary of properties. \n",
    "\n",
    "    Args:\n",
    "        event (string): The XML string representing the Windows event log entry.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of properties extracted from the XML string.\n",
    "    \"\"\"\n",
    "    tree = ET.ElementTree(ET.fromstring(event))\n",
    "    root = tree.getroot()\n",
    "    ns = \"{http://schemas.microsoft.com/win/2004/08/events/event}\"\n",
    "    data = {}\n",
    "    for eventID in root.findall(\".//\"):\n",
    "        if eventID.tag == f\"{ns}System\":\n",
    "            for e_id in eventID.iter():\n",
    "                if e_id.tag == f\"{ns}System\":\n",
    "                    pass\n",
    "                elif e_id.tag == f\"{ns}Provider\":\n",
    "                    data[\"Provider\"] = e_id.attrib.get('Name')\n",
    "                elif e_id.tag == f\"{ns}TimeCreated\":\n",
    "                    data[\"TimeCreated\"] = e_id.attrib.get('SystemTime')\n",
    "                elif e_id.tag == f\"{ns}Correlation\":\n",
    "                    data[\"ActivityID\"] = e_id.attrib.get('ActivityID')\n",
    "                elif e_id.tag == f\"{ns}Execution\":\n",
    "                    data[\"ProcessID\"] = e_id.attrib.get('ProcessID')\n",
    "                    data[\"ThreadID\"] = e_id.attrib.get('ThreadID')\n",
    "                elif e_id.tag == f\"{ns}Level\":\n",
    "                    if not int(e_id.text) in evt_level_dict.keys():\n",
    "                        data['Level'] = \"unknown\"\n",
    "                    else:\n",
    "                        data['Level'] = evt_level_dict[int(e_id.text)]\n",
    "                elif e_id.tag == f\"{ns}Opcode\":\n",
    "                    if not int(e_id.text) in evt_opcode_dict.keys():\n",
    "                        data['Opcode'] = \"unknown\"\n",
    "                    else:\n",
    "                        data['Opcode'] = evt_opcode_dict[int(e_id.text)]\n",
    "                else:\n",
    "                    att = e_id.tag.replace(f\"{ns}\", \"\")\n",
    "                    data[att] = e_id.text\n",
    "\n",
    "        if eventID.tag == f\"{ns}EventData\":\n",
    "            for attr in eventID.iter():\n",
    "                if attr.tag == f'{ns}Data':\n",
    "                    if attr.get('Name') is None:\n",
    "                        data[\"Data\"] = attr.text\n",
    "                    else:\n",
    "                        data[attr.get('Name')] = attr.text\n",
    "                elif attr.tag == f'{ns}Binary':\n",
    "                    data[\"Binary\"] = attr.text\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def new_logs_event_handler(reason, context, evt):\n",
    "    \"\"\"\n",
    "    Called when new events are logged.\n",
    "\n",
    "    reason - reason the event was logged?\n",
    "    context - context the event handler was registered with\n",
    "    evt - event handle\n",
    "    \"\"\"\n",
    "    # Just print some information about the event\n",
    "    # print ('reason', reason, 'context', context, 'event handle', evt)\n",
    "\n",
    "    event = win32evtlog.EvtRender(evt, win32evtlog.EvtRenderEventXml)\n",
    "    result = \" \".join(l.strip() for l in event.splitlines())\n",
    "    log = parse_XML_log(event=result)\n",
    "    with open('xml_logs_test.txt', 'a') as file:\n",
    "        file.write(str(log))\n",
    "        file.write('\\n')\n",
    "    # my_producer.send('users', value=result)\n",
    "    print(' New log record! ')\n",
    "\n",
    "    # Make sure all printed text is actually printed to the console now\n",
    "    sys.stdout.flush()\n",
    "    return 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    kafka_config = read_config('../kafka_config.ini')\n",
    "    producer = KafkaProducer(bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "                             value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "    print('Hello - Welcome to my Windows Logs Collector!!!')\n",
    "    subscription1 = win32evtlog.EvtSubscribe('application', win32evtlog.EvtSubscribeToFutureEvents,\n",
    "                                             None, Callback=new_logs_event_handler, Context=event_context, Query=None)\n",
    "    subscription2 = win32evtlog.EvtSubscribe('system', win32evtlog.EvtSubscribeToFutureEvents,\n",
    "                                             None, Callback=new_logs_event_handler, Context=event_context, Query=None)\n",
    "    subscription3 = win32evtlog.EvtSubscribe('Security', win32evtlog.EvtSubscribeToFutureEvents,\n",
    "                                             None, Callback=new_logs_event_handler, Context=event_context, Query=None)\n",
    "    while True:\n",
    "        sleep(10)\n",
    "        os.rename('xml_logs_test.txt', 'xml_logs_done.txt')\n",
    "        with open('xml_logs_done.txt', 'r') as f:\n",
    "            data = f.readlines()\n",
    "            producer.send(topic=kafka_config['topic_name'], value=data)\n",
    "        os.remove('xml_logs_done.txt')\n",
    "        if msvcrt.kbhit() and msvcrt.getch() == chr(27).encode():\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not pyuac.isUserAdmin():\n",
    "        print(\"Re-launching as admin!\")\n",
    "\n",
    "        pyuac.runAsAdmin()\n",
    "        input(\"Press enter to close the window. >\")\n",
    "    else:\n",
    "        main()  # Already an admin here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_config = read_config('../kafka_config.ini')\n",
    "producer = KafkaProducer(bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "                             value_serializer=lambda x: dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send(topic=kafka_config['topic_name'], value=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        sleep(10)\n",
    "        try:\n",
    "            os.rename('xml_logs_test.txt', 'xml_logs_done.txt')\n",
    "            with open('xml_logs_done.txt', 'r') as f:\n",
    "                data = f.readlines()\n",
    "                producer.send(topic=kafka_config['topic_name'], value=data)\n",
    "            os.remove('xml_logs_done.txt')\n",
    "            \n",
    "        except:\n",
    "        \n",
    "            if msvcrt.kbhit() and msvcrt.getch() == chr(27).encode():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('xml_logs_test.txt', 'xml_logs_done.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('xml_logs_done.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
